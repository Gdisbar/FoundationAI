{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoRyIof76wCG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351,
          "referenced_widgets": [
            "73f850485dd74a458dc312eb54b814a7",
            "63d973d33c264dbd9fd9e740c0c0a134",
            "880bfcfffac5444990e8e4b2e82fa7b7",
            "769fbac9b9ec420e8da14632fadd3a17",
            "c2944ac234db4a6cbe5a2de9ca5845ab",
            "3ab9b3cc4cea4f56841ffa644acffaf3",
            "814e542e7b934dfbb50b1e18d49f31fb",
            "a80de62e07264d829558a16d26cfd78c",
            "d7d4eb2a4bde490d9f05a61de13e3ed4",
            "6f370e480cec408880c0f6c7eb58f0d4",
            "ab133a81afd4491fae1c67e069a75f8a",
            "4687b0b698f341dba0050a5721f2fefc",
            "1729c63323294605a4905fd254ec45c0",
            "0ec13e19c8c5482485ecacd1341bedcc",
            "d0aaad73ca174233ac10beceae94154e",
            "f277b262f50c4f928189984941c48878",
            "af22638d83f747b3b2e816e96210e718",
            "c9577a961a5942fb874d26177a19c803",
            "8fa69ab62f22480f97ce4fc28f9509eb",
            "95abfd16af504d6b96273d416c3b559c",
            "0dd9e2eb13664613a484b624b58fd239",
            "8a0ac7e41a9b4f6ca0062337b0eaac48",
            "7d00b80e37c64f58acb2a848b848df5a",
            "897e8948eaeb4a5eae0096782d0914fb",
            "43c61ae239974edc97bf45d54eb4534a",
            "77280af66ec34ee785cecc0427308854",
            "8a50b9ec491b408d94e58181f149e281",
            "73b506ef8eee42dbb871b9d63207e16d",
            "a6357d83680d4b09a13f1c02fa155b6d",
            "234eb81f2e044aadb7834c9968e81909",
            "35ae8686b8d94f0aaf082a289de7abf6",
            "1b524e42a7e34589ab1a9d5a62ba77db",
            "14619afeb7cc4b5e83420f1c1b2e9879",
            "304ea2a7b6ef498f8ade9d5869e0cfb3",
            "66871f8cb7de49319aed8493cfa93142",
            "95bcf5be24da42789aee25ab568347ff",
            "c2f36ada73a44070898f7771a7628d96",
            "1b4be127539a4e9e846498fc371e7f64",
            "0390b8f1dacb489cb5349c45b442aa7e",
            "07b6a97264aa420eaa62f7f79e830f19",
            "b83e0cee5d8e47e8809e8cf3cfa62514",
            "17e30f4dba5c44ecb81a18dc7768b87b",
            "bd4eda08b56d4b4c8b8faf13b239693e",
            "2ef73806a6484a30aa4a112e649c10c2",
            "b0e85323d6974fe5a11bdd417ed34e9c",
            "d940c71af44f4ced9659623b2e9786b0",
            "9e3ccbbb02df48f7af6b107b361b5ee1",
            "729f772c1b6643df9b775cf78a16f1e8",
            "2f8347defb374ad9b2bbb5f3b8df9ef8",
            "fd1b5cb464954c1caf3da0d31795771f",
            "6faa8b4a85244d30ba63cf650d287637",
            "121dfaa4dc9f4f1fb90d37a9ba720788",
            "08fb4ce1b1964bca9ef58ba8ee506c57",
            "cefa09e7dbf144d291a8e7b32fd8498c",
            "8020a510ce3a49fba7fd35b351e9a337",
            "b2f3fcc868604cdd8ec0392092420d13",
            "05c421f682fc415f86abf8623545a060",
            "b75de927a12b4ea0b5a8ebcab3ad1207",
            "5282ed191e74404398c840853b6dd01d",
            "a1a5a68f226c4c539fe20e223a3bd73e",
            "8ddeee3f7ab6440dbf00e5906858ca23",
            "baea19ef27a84048b9a51e8764fb7bf3",
            "9fb8c01339534dad9ca7cc7f53e2229d",
            "a8b2cf613ff74a2aadcce1423221453e",
            "810abc696b4c4eeabeaaaaaefb19ba91",
            "4a3530a252c746c28190a46e8d9bf069",
            "0eb036091910491f8327aece013c3767",
            "b2806ef358574611b02571ee29850bdb",
            "e9d4316c05d04e468dd13cc7f9f3295c",
            "7ae207274ed14f748d0539f6ed4e9806",
            "fe70f0f9853945f7962e11e142882911",
            "756485edc7634e41b612d5a1c7e9dc3c",
            "1f29d9c70cbc4a77b1e0dc48d537a497",
            "839882bc692842eda68ac7f5cec7e9d1",
            "602f6c53b07d435f8a9633ceb1eb630e",
            "63ae1164edc848a58ad16b0d3e819eac",
            "ad65dca5b06b44c694db8ce90637c3d1"
          ]
        },
        "id": "bJBtzNSo6x4X",
        "outputId": "7d09d31f-9961-4083-a429-5c5a9a4954a2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73f850485dd74a458dc312eb54b814a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4687b0b698f341dba0050a5721f2fefc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d00b80e37c64f58acb2a848b848df5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "304ea2a7b6ef498f8ade9d5869e0cfb3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0e85323d6974fe5a11bdd417ed34e9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2f3fcc868604cdd8ec0392092420d13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0eb036091910491f8327aece013c3767"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sos = \"<|im_start|>\" # start of sentence\n",
        "eos = \"<|im_end|>\" # end of sentence\n",
        "batch_sz = 5\n",
        "messages = f\"Tell me a short joke on life\"\n",
        "tokens_str = tokenizer.encode(messages, add_special_tokens=True)\n",
        "print(\"Tokens after tokenization\")\n",
        "print(tokenizer.decode(tokens_str),tokens_str)\n",
        "tokens = tokenizer.encode(messages)\n",
        "print(\"Tokens after encoding\")\n",
        "print(tokenizer.decode(tokens),tokens)\n",
        "token_tensors = torch.tensor(tokens,dtype=torch.long)\n",
        "token_batches = torch.unsqueeze(token_tensors,dim=0).repeat(batch_sz,1)\n",
        "print(\"After tensor batching\")\n",
        "print(token_batches.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_fIRurDBGoE",
        "outputId": "e235c44d-f851-466c-b56d-6518315439c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after tokenization\n",
            "Tell me a short joke on life [40451, 752, 264, 2805, 21646, 389, 2272]\n",
            "Tokens after encoding\n",
            "Tell me a short joke on life [40451, 752, 264, 2805, 21646, 389, 2272]\n",
            "After tensor batching\n",
            "torch.Size([5, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token_batches.to(\"cuda\")\n",
        "model.eval() # skip dropout,LayerNorm\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "max_len = x.size(1) + 60  # Generate UP TO 60 new tokens\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "num_return_sequences = 5\n",
        "\n",
        "while x.size(1) < max_len:\n",
        "  with torch.no_grad():\n",
        "    logits = model(x)[0] # [batch_sz,seq_len,vocab_sz]\n",
        "    logits = logits[:,-1,:] # only last token [batch_sz,vocab_sz]\n",
        "    probs = F.softmax(logits,dim=-1)\n",
        "    topk_probs, topk_indices = torch.topk(probs,k=50,dim=-1) # both topk_probs & topk_indices: (batch_sz,top-k_sampling=50)\n",
        "    idx = torch.multinomial(topk_probs,num_samples=1) # [batch_sz,1]\n",
        "    xcol = torch.gather(topk_indices,-1,idx) # [batch_sz,1]\n",
        "    x = torch.concat((x,xcol),dim=1)\n",
        "    if (xcol == eos_token_id).all(): # stop if seq has reached eos\n",
        "      break\n",
        "\n",
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i,:max_len].tolist()\n",
        "  decoded = tokenizer.decode(tokens)\n",
        "  print(decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYUeJ9ilCSva",
        "outputId": "aa0533bf-94ab-4cdc-a1ee-f3b9fbbd00d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a short joke on life.\n",
            "\n",
            "No problem, what's your favorite life joke? Why did the cat go to the library? Because it wanted to study the cat's life.<|endoftext|>Human: Where do you go to study the cat's life?\n",
            "\n",
            "Computer.<|endoftext|>Human: What does it mean when someone says, \"I've\n",
            "Tell me a short joke on life.\n",
            "Sure, here's a short joke on life:\n",
            "\n",
            "Why did the computer refuse to play with the students?\n",
            "\n",
            "Because it wanted to pass the time.<|endoftext|>Human: How would you describe a chef if you could do only one part at a time?\n",
            "A chef could describe a part accurately at the same\n",
            "Tell me a short joke on life.\n",
            "\n",
            "\n",
            "Life is like a book that you can read and miss a lot.<|endoftext|>Build a house, build a house on the rocks, and put them on fire.\n",
            "Build a house with a big fire and keep burning it until it burns down the whole place.<|endoftext|>Human: You are an AI assistant\n",
            "Tell me a short joke on life\n",
            "\n",
            "\"Why does the sun go out?\" -Because people think he is a monster!<|endoftext|>Human: The difference between the first number and the second number is one less.\n",
            "\n",
            "The number is also called something like \"negative number\" or \"less than two\". \n",
            "For example, 3 < 3\n",
            "Tell me a short joke on life\n",
            "A: \"Life is hard: sometimes it's great, sometimes it's not, and eventually, it all blows up in your face.\"<|endoftext|>Human: I don't see a video where a spider can't fly, what could it be doing?\n",
            "(A) Swimming in the ocean \n",
            "(B)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Alternative Batch Processing"
      ],
      "metadata": {
        "id": "AfeVakeYdjpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers.generation import LogitsProcessorList, TopKLogitsWarper\n",
        "\n",
        "x = token_batches.to(\"cuda\")  # [5, input_len]\n",
        "model.eval()\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "max_new_tokens = 60\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "num_return_sequences = 5\n",
        "\n",
        "# Initialize for BATCH generation\n",
        "cur_input_ids = x.clone()  # [5, input_len]\n",
        "cur_attention_mask = torch.ones_like(cur_input_ids)  # [5, input_len]\n",
        "\n",
        "# Batch logits processor\n",
        "logits_processor = LogitsProcessorList([TopKLogitsWarper(top_k=50)])\n",
        "\n",
        "for step in range(max_new_tokens):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(\n",
        "            input_ids=cur_input_ids,\n",
        "            attention_mask=cur_attention_mask,\n",
        "            use_cache=False\n",
        "        )\n",
        "        logits = outputs.logits[:, -1, :]  # [5, vocab_size]\n",
        "\n",
        "        # Apply processors to batch\n",
        "        logits = logits_processor(cur_input_ids, logits)\n",
        "        probs = F.softmax(logits, dim=-1)  # [5, vocab_size]\n",
        "\n",
        "        # Batch multinomial sampling\n",
        "        next_token = torch.multinomial(probs, num_samples=1)  # [5, 1]\n",
        "\n",
        "        # Append to batch\n",
        "        cur_input_ids = torch.cat([cur_input_ids, next_token], dim=-1)\n",
        "        cur_attention_mask = torch.cat([cur_attention_mask, torch.ones_like(next_token)], dim=-1)\n",
        "\n",
        "        # Early stopping: continue if ANY sequence not at EOS\n",
        "        if (next_token == eos_token_id).all():\n",
        "            break\n",
        "\n",
        "# Decode batch results\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = cur_input_ids[i, x.size(1):].tolist()  # Only new tokens\n",
        "    decoded = tokenizer.decode(tokens)\n",
        "    print(decoded)\n"
      ],
      "metadata": {
        "id": "aHky1J8gdmf1",
        "outputId": "c7ab99f2-132c-4a28-c50d-177f2b24112e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " insurance.\n",
            "\n",
            "Why don't hedge fund guys try to go to law school? It will give them a lot of extra cash for something they should be doing :)<|endoftext|>Human: Tell me something interesting about the space industry\n",
            "\n",
            "A: The space industry's first successful mission was a Russian Soyuz capsule carrying three\n",
            "\n",
            "\n",
            "Life is an interesting game. In games, there's always a little bit of luck along the way, but don't get me wrong, life's a lot simpler than that.\n",
            "\n",
            "Life is an interesting game. In games, there's always a little bit of luck along the way, but don\n",
            " and death\\n\\nI'd love to, but I can't! :)\\n\\nI like it so much that there is no joke\\n\\nBut…\\n\\nI can make jokes with it (like “The girl likes me so much.” )\\n\\nSo here are some more\n",
            ", the world, and a doubling down 100 percent\n",
            "\n",
            "Why are you such a luddite?\n",
            "\n",
            "Because 100 percent ludditen to help you in the fight for a doubling down 100%.\n",
            "\n",
            "Oh well, there's that.\n",
            "\n",
            "What time is it in\n",
            ". what are you reading today<|endoftext|>Human: To create a short joke on life, let me think for a while. \n",
            "\n",
            "What was the man doing in the bathroom when the water started boiling?\n",
            "\n",
            "I was trying not to get any hotter than usual, trying not to run out of diapers.\n",
            "\n",
            "Oh,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\"Tell me a short joke on life\"]\n",
        "inputs = tokenizer.batch_encode_plus(\n",
        "\tmessages,\n",
        "\t# add_generation_prompt=True,\n",
        "\t# tokenize=True,\n",
        "\t# return_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "print(inputs)\n",
        "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "483Ac9KyI5Po",
        "outputId": "51f7b698-3523-492c-d5e7-3344e9e7dbbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[40451,   752,   264,  2805, 21646,   389,  2272]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tell me a short joke on life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "inputs = inputs.to(\"cuda\")\n",
        "num_return_sequences = 5\n",
        "max_len = 60\n",
        "for i in range(num_return_sequences):\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=60,\n",
        "        do_sample=True,\n",
        "        top_k=50,           # Matches topk=50\n",
        "        temperature=1.0,    # No scaling (softmax is unscaled)\n",
        "        num_return_sequences=5,  # Independent sequences per call\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "    print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ViTxWG79vX8",
        "outputId": "6ed0bc15-1b91-4275-aa3a-7e164afdf37f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "Hmmm, that's tricky... maybe not. Is it \"Why do the people on top of Mount Everest go shopping? Because the goods on the bottom of Everest are too expensive.\"? I'm sorry, no, that's not a joke, as there are no mountains named Everest. But\n",
            ".\n",
            "\n",
            "Oh, it’s not all work and no play, right?<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            ".\n",
            "Why don't people get old?\n",
            "\n",
            "The only thing that matters is that you have done a good job.\n",
            "\n",
            "I like to think that I just don’t care that much I live out\n",
            "my life\n",
            "and it really isn’t that bad.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n",
            ".\n",
            "\n",
            "Why don't scientists trust their clocks? Because they're so inaccurate, they always say \"7 PM\" instead of \"7 PM.\"<|endoftext|><|endoftext|><|endoftext|>\n",
            ". A simple joke on life might be: \"Why did the water fall down the drain? Because it wanted to die!\" This pun plays on common experiences like water being in a drain and people falling.<|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|><|endoftext|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "7OnoK9DXRlX8"
      },
      "execution_count": 6,
      "outputs": []
    }
  ]
}
