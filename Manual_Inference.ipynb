{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoRyIof76wCG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241,
          "referenced_widgets": [
            "90dd4e41546f4a40820466e79890eee6",
            "eb26ce8f5b204dceb75320d6ac3d694e",
            "05b41f45ce154b7fa0271fec8a41b99b",
            "f696d2f7eabd4c23b66b9db9ec6202d7",
            "678f41060be6466c9d4b380be11201ea",
            "8ddf938788694914b6ac04d0d7ef0335",
            "33a7f037f5e94b9590cc354a82a9e8c3",
            "d64367b730864708ac1467262d61b080",
            "d2c09e1ba8c04926a3aff13246561e7e",
            "9921327b5c954554873cdd2e39e854f7",
            "81e8655bd31e4378823dc804430acf21",
            "76822daeddfa4918a80def8a3024ef14",
            "5a91307674dc416c925d07e354fad3a5",
            "5b6010ae7c1d4cfdbee95240e4418487",
            "1736bb258f08432cb3205e2b024a490f",
            "99055acec8784bb4a81e2eff83b8005a",
            "aef2c98e7cd544e6b560f5c3e3fd2440",
            "f48530f7dea24398a7c3aa42ce406436",
            "e5e61805982d4e1eb0a1fc30ded129a0",
            "eeea6bacf2434dd0b6904f63ed02f404",
            "8fa37ee44381448685a7edc9ca7e4613",
            "65a633533a23493180277ac0113d6e17",
            "7d2f204f4efc477199b618e19e2292c5",
            "3e74bc787d6b40b391c0904bdc526f7b",
            "0edec1fa22c944daa7170f755cd6fdc0",
            "22506885b64c443bac292f208e124c92",
            "d4664b31f0104de7ae023ddde32c7fa0",
            "3f9f3536d9734b59923dc9040df7f423",
            "6c43615c34124776bc3289a84232ae82",
            "f4d9e62feb1e4b01b69cf28a6fb634d3",
            "cc366571a8b54cd2add06180a94f7971",
            "75420e06bb7d4d3daab06f7021b080a2",
            "abd0abe5f12c4dedb9809ec2beda1bf0",
            "f84ac20864d9471c822063b8261c343e",
            "f7a568aca580425bb2c7ccb46d22daa6",
            "0deb6730966947dab1c22d5d5ef8acaa",
            "ac0ad1bd0dc9406486c72d7f1b267449",
            "4ff94dd01e4a4004a6caa5be5410f61b",
            "eaf0b21618684fad8e80c286d125f69a",
            "5eeaabee9fc44dbf8bd1f6823f487aa5",
            "5c396f7c24e448fe823abd8b8c31d428",
            "4281c0e3f2d14ac3af07acfad3703767",
            "d6733cf24a7f440a975e2bcf112c9345",
            "a3d49e3016fa4e7fbd7386871f57c6c5",
            "ae46f3710e504859992061c1ab77183c",
            "d35f9d8b2d1d4be7a7d06a1f69f93522",
            "36dadc14927f4970ab3fb68bf3d253d6",
            "d730a91454544e02a101415b38e87e68",
            "d98d83d3d41c40f48d5676db511364c7",
            "5dea68c4a0cf46d3a4a409ac689b2d58",
            "883d95817c3943968f5606709044bf99",
            "2c1b1c1b9ed442698e2925efdf26dcde",
            "bfd2ec66b69c437f82d778224bb20fdd",
            "03ab75047b9e4749a3ad9a161789563e",
            "fa58c25c75b24250aace57438fde2a5a",
            "328b89f2bb0148b99e47fd6c31278bc2",
            "7d5850b542ac467080319eb85caf5eae",
            "5917ebde28a444c4838bbea6cba10cab",
            "4046841acda04348aaf2e0cd32019b87",
            "ec6e1dc72c1e46c68d20cc0b21786e41",
            "4a0fc2a6b8b44a4f9de2702f376f6c64",
            "4836fc613412490ab058cc1b4bee66ca",
            "933eda3434694a9e8eeedacff2d094e6",
            "8e554abfd0864cdb88f3df69c92390a7",
            "df0aba74554548c3817e4499d2b2c4b1",
            "701ac5b918da4470b94b579195e974f9",
            "80060731f981403a8bd9e17ff8b11982",
            "95eab9bd6d2f42009002e532475d164e",
            "02f4371dbeaf44bcb7dc7fde8d9f63f6",
            "013ce70d624a49c486ae9a45bfc686bd",
            "ffeabe20c5e34776a1ba316edba3e2fe",
            "fbdea41954ba4749864e9c2b97be7e75",
            "6e88a96281254f94b1ae79d4e5198fdd",
            "2eb6c3a3149d4e93998dd452e99317d2",
            "45b3d2009d29476e973120666823364e",
            "55d2581c82494ae7927063e96526767d",
            "85d22fedc6734b04a2bd50facfdca1d4"
          ]
        },
        "id": "bJBtzNSo6x4X",
        "outputId": "4c84932d-e313-4403-e327-495ea9e5eaf6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "90dd4e41546f4a40820466e79890eee6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76822daeddfa4918a80def8a3024ef14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d2f204f4efc477199b618e19e2292c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f84ac20864d9471c822063b8261c343e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae46f3710e504859992061c1ab77183c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "328b89f2bb0148b99e47fd6c31278bc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80060731f981403a8bd9e17ff8b11982"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "sos = \"<|im_start|>\" # start of sentence\n",
        "eos = \"<|im_end|>\" # end of sentence\n",
        "batch_sz = 5\n",
        "messages = f\"Tell me a short joke on life\"\n",
        "tokens_str = tokenizer.encode(messages, add_special_tokens=True)\n",
        "print(\"Tokens after tokenization\")\n",
        "print(tokenizer.decode(tokens_str),tokens_str)\n",
        "tokens = tokenizer.encode(messages)\n",
        "print(\"Tokens after encoding\")\n",
        "print(tokenizer.decode(tokens),tokens)\n",
        "token_tensors = torch.tensor(tokens,dtype=torch.long)\n",
        "token_batches = torch.unsqueeze(token_tensors,dim=0).repeat(batch_sz,1)\n",
        "print(\"After tensor batching\")\n",
        "print(token_batches.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_fIRurDBGoE",
        "outputId": "99eedd8e-e87b-41f0-865b-fa1866b3bfac"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after tokenization\n",
            "Tell me a short joke on life [40451, 752, 264, 2805, 21646, 389, 2272]\n",
            "Tokens after encoding\n",
            "Tell me a short joke on life [40451, 752, 264, 2805, 21646, 389, 2272]\n",
            "After tensor batching\n",
            "torch.Size([5, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = token_batches.to(\"cuda\")\n",
        "model.eval() # skip dropout,LayerNorm\n",
        "model = model.to(\"cuda\")\n",
        "\n",
        "max_len = x.size(1) + 60  # Generate UP TO 60 new tokens\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "num_return_sequences = 5\n",
        "\n",
        "while x.size(1) < max_len:\n",
        "  with torch.no_grad():\n",
        "    logits = model(x)[0] # [batch_sz,seq_len,vocab_sz]\n",
        "    logits = logits[:,-1,:] # only last token [batch_sz,vocab_sz]\n",
        "    probs = F.softmax(logits,dim=-1)\n",
        "    topk_probs, topk_indices = torch.topk(probs,k=50,dim=-1) # both topk_probs & topk_indices: (batch_sz,top-k_sampling=50)\n",
        "    idx = torch.multinomial(topk_probs,num_samples=1) # [batch_sz,1]\n",
        "    xcol = torch.gather(topk_indices,-1,idx) # [batch_sz,1]\n",
        "    x = torch.concat((x,xcol),dim=1)\n",
        "    if (xcol == eos_token_id).all(): # stop if seq has reached eos\n",
        "      break\n",
        "\n",
        "for i in range(num_return_sequences):\n",
        "  tokens = x[i,:max_len].tolist()\n",
        "  decoded = tokenizer.decode(tokens)\n",
        "  print(decoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYUeJ9ilCSva",
        "outputId": "3be69cba-b12a-41e8-8de8-0ece22bbc2a4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tell me a short joke on life\n",
            "\n",
            "Sure, I can come up with a short joke on life.\n",
            "\n",
            "Why are you going to die?\n",
            "Because there are more snakes than sharks, and you can't eat them all.\n",
            "\n",
            "And what's this \"JAP! JAP!\" sign on the door?\n",
            "It means \"Don't Eat This\n",
            "Tell me a short joke on life.\\nIf life could have a book, I'd have one dedicated to life itself. Why? A book about life. What about a book about life itself?\n",
            "There can be no such thing as the most boring life.\n",
            "You can't ask a question if you don't want to be answered.\n",
            "Tell me a short joke on life and death. Sure, here's a short one:\n",
            "\n",
            "Why don't scientists like deaths?\n",
            "\n",
            "Because they could be like time machines! \n",
            "\n",
            "(You get a chuckle from the audience!)<|endoftext|>Choose from: (a). Yes; (b). No;\n",
            "\n",
            "Sally said, \"When is my lunch\n",
            "Tell me a short joke on life\n",
            "\n",
            "Life is like climbing a mountain, sometimes it's beautiful, other times it's just an uphill struggle, and it's best to take a step into the woods.<|endoftext|>Human: Write me a short story\n",
            "\n",
            "Once upon a time, there was a young girl named Lily who loved animals. She\n",
            "Tell me a short joke on life. Sure, here's one:\n",
            "\n",
            "Why do scientists go to sleep early? Because they want to study aliens!<|endoftext|>How do you say \"Get your ass up\" in Japanese? Ah ... テスト\n",
            "\n",
            "If I want you to translate a sentence to Japanese, when would I use the 'た\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\"Tell me a short joke on life\"]\n",
        "inputs = tokenizer.batch_encode_plus(\n",
        "\tmessages,\n",
        "\t# add_generation_prompt=True,\n",
        "\t# tokenize=True,\n",
        "\t# return_dict=True,\n",
        "\treturn_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "print(inputs)\n",
        "print(tokenizer.decode(inputs[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "483Ac9KyI5Po",
        "outputId": "f8d3111c-0f5b-442d-b5c9-cab03e987616"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[40451,   752,   264,  2805, 21646,   389,  2272]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
            "Tell me a short joke on life\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = model.to(\"cuda\")\n",
        "inputs = inputs.to(\"cuda\")\n",
        "num_return_sequences = 5\n",
        "max_len = 60\n",
        "for i in range(num_return_sequences):\n",
        "    outputs = model.generate(**inputs, max_new_tokens=max_len,do_sample=True)\n",
        "    print(tokenizer.decode(outputs[0][inputs[\"input_ids\"].shape[-1]:]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ViTxWG79vX8",
        "outputId": "73359228-390c-49e6-810e-b56121b81389"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\\nLife is like a restaurant, you can eat whatever you want. But sometimes, it costs a bit more, if you wish to order it at the first time.\\nLife may be about saving and finding a balance...<|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " problems.\n",
            "A person goes to a shop and asks if they can get 2 bags of bread, they have a bag of wheat. \n",
            "\n",
            "Do you think your boss would like to find another colleague?\n",
            "\n",
            "The boss is sure it's an obvious answer and will immediately fire you.<|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".\n",
            "\n",
            "Why are people on life, so often on vacation? Because life doesn't have a vacation!<|endoftext|>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The phrase 'Life is not a game' has been used by many people to express a sense of pessimism or despair, and it's certainly not meant as a joke. However, if you're looking for a short and amusing one, there's a good chance that \"Life ain't a game\n",
            ".\n",
            "Why doesn't life give a rat to a pig like a pig? Because the pig has no memory!\n",
            "\n",
            "That's my favorite joke! Do you have any other short stories or jokes that I can get stuck on?\n",
            "Sure, I'll try my best. Here's a short joke on the internet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "7OnoK9DXRlX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
