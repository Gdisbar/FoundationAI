{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbpX9vSMpToi",
        "outputId": "5cf05ef0-4131-4856-cf29-f5d3aa7017e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.8 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade pip\n",
        "!pip install -q --upgrade transformers datasets[audio] accelerate torchcodec\n",
        "# !pip install -q flash-attn --no-build-isolation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzhiQ7cQpUeR",
        "outputId": "8c07ec03-0a35-45e4-acd2-68d25ffc8b07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# from transformers.utils import is_torch_sdpa_available\n",
        "\n",
        "# print(is_torch_sdpa_available())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "WhisperForConditionalGeneration(\n",
        "  (model): WhisperModel(\n",
        "    (encoder): WhisperEncoder(\n",
        "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
        "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
        "      (embed_positions): Embedding(1500, 768)\n",
        "      (layers): ModuleList(\n",
        "        (0-11): 12 x WhisperEncoderLayer(\n",
        "          (self_attn): WhisperSdpaAttention(\n",
        "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
        "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "          )\n",
        "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (activation_fn): GELUActivation()\n",
        "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
        "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
        "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "      )\n",
        "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "    )\n",
        "    (decoder): WhisperDecoder(\n",
        "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
        "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
        "      (layers): ModuleList(\n",
        "        (0-11): 12 x WhisperDecoderLayer(\n",
        "          (self_attn): WhisperSdpaAttention(\n",
        "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
        "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "          )\n",
        "          (activation_fn): GELUActivation()\n",
        "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (encoder_attn): WhisperSdpaAttention(\n",
        "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
        "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "          )\n",
        "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
        "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
        "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "      )\n",
        "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "    )\n",
        "  )\n",
        "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
        ")\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Ug2wAYCvLy9q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OMca3Yz71dVv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d82397a5-7ec8-4b6c-b286-b0eddaadb0d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor\n",
        "from datasets import Audio, load_dataset\n",
        "\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3702ea38863b4bcdbe37f6d0fd81848d",
            "2fc4386a29f649b1adf43ef62996e170",
            "823222f4c1614595922c61b8f13425f1",
            "06502429b1fa45f09468ac88455e3c1c",
            "1364e2a363ad436890c1dbd2cae8a66b",
            "40e288a555474b9abf4339a68da9fd9c",
            "1d0fa53e1ee34073a58c99e784e88c55",
            "e45dc300c4ae4e6d806e2244ce120148",
            "36b29aba94c7498f86bb3ee9da51d39a",
            "f5c144522b204b85a998a65a3d009ead",
            "47ab9cd013144447b7f382bce4cfae70",
            "177b5677f5534ed5b1e74d5ce081278c",
            "d1908660bf1f43a9ac70b33963340c59",
            "41f8b52b3e75426baedf0cad4443242c",
            "9d605145fb2b4ff5bf70f52c5ade8d22",
            "73c695feb7c843f1b324222b832b2d04",
            "668c8f83aef347c98b567e817e9dc50c",
            "31f8d030a6e446a0bf3eff6cd2c69c1c",
            "fe56c202bfe344cb9366d45283b43e12",
            "698a1c6439e0456aae5d1c97e79c0276",
            "4458251b6aea4d699f9bbba8d1d5025e",
            "a532897b05954b45a70196749439cfde"
          ]
        },
        "id": "WWqYqaHcpUia",
        "outputId": "6b96f94c-4cca-45d9-fb4c-1f919fe8fa67"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3702ea38863b4bcdbe37f6d0fd81848d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "177b5677f5534ed5b1e74d5ce081278c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WhisperForConditionalGeneration(\n",
              "  (model): WhisperModel(\n",
              "    (encoder): WhisperEncoder(\n",
              "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (embed_positions): Embedding(1500, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperEncoderLayer(\n",
              "          (self_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): WhisperDecoder(\n",
              "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
              "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperDecoderLayer(\n",
              "          (self_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\n",
        "model_id = \"openai/whisper-small\"  # whisper-large-v3\n",
        "\n",
        "# Scale-Product-Attention (SDPA) - can also use flash_attention_2\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True,\n",
        "    attn_implementation=\"sdpa\")\n",
        "\n",
        "model.to(device)\n",
        "# model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4k4zm8UrzxT2"
      },
      "outputs": [],
      "source": [
        "# Enable static cache and compile the forward pass\n",
        "# torch.compile is currently not compatible with the Chunked long-form algorithm or Flash Attention 2\n",
        "model.generation_config.cache_implementation = \"static\"\n",
        "model.generation_config.max_new_tokens = 256\n",
        "# model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Q_XQEYkwrsiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "6c89218112644c7e834a9cbd16f35832",
            "ce5cc5b5302149c59d0d405515eab138",
            "0198cb0fd01345018ff52d5cfc5e0034",
            "4050a05de92548cca32176944efd2c78",
            "6928b2b3e09649bab17821a89739552c",
            "ef61606c7e4c4f53862a56a6bca79926",
            "d484480ab2f1451fb4a5d5ada17d8539",
            "eaba24ef94c84c3394bb29ea82cf0b2b",
            "946eebca1d7d490293923055543786f4",
            "72d29b2ffd5a4977b1dee850b23c6ee4",
            "a680023daf02467492711cc5c80b909c",
            "15df5ab7c5f847cfb91e05ffffe18a98",
            "d674f4e64e584e07bbe2ff16690d86a0",
            "e8ce41bd5e1e4e25ae6183ba18615eea",
            "2fb2d510e89c44298800e7887b2bcf85",
            "e4a9252f8b054a78a0919bbb2b149685",
            "613a973fc4344f61921e9ac079d8e811",
            "be3879b304bb44d98ee5ab00acd47e24",
            "3f22295674ab4b02acb3400706a1f2e8",
            "0b892b50ee4a4af5b04ca716d3b83ec0",
            "45afc23052d844fcadfa9e423f147071",
            "41adfa1074154d7f87769a1098f00f92",
            "cd18c2fdd6b34bb79ffaa6e8b2aea6f2",
            "a9bcbd7e65724ac28244a28cf0421dc7",
            "976e26e8cb46435a9c17d9493780677d",
            "ce45c99f4bdd43b89948104077af7110",
            "8cdef5561a334694a3de098a0a343484",
            "7e1c086a33c841bab7afcaa5570d0f65",
            "291bb993a9954378a64c969ad1f2b826",
            "d56b8804aea94db783abdfc0d43dd439",
            "6c54b07cc58441669690e2b7243f20ee",
            "43a25dd75f7e44bd918f27a505236982",
            "b232580595f64ef08aaac0eef2532468",
            "fa2c5e53826e404fb415e4a4cb04fa14",
            "693a8897f518488daa907bd637f79251",
            "64dd48e8c57242efa3c9006f6e86b853",
            "db9d785ef9a64f91aba383a16d605f9e",
            "d9c1ae0e57c749f6bd49546331f7a937",
            "b8fabf3acdcb4fc1adc06ccd017416d6",
            "f2c479ed9cb54cd09e7a5efa4793aac0",
            "b956dba96b8746bd908782440406bf13",
            "dfe31cd24bfd4ce7aca6210d628763a0",
            "7843d3a6328d44f19c1e3df9656e3e34",
            "b1386edd55874c57aff3acefbe329f43",
            "60530f2909704f11bd0965dade1de13a",
            "56a3463247994638b28b5780e9615c66",
            "ba46541f9b63462eb32b891453af3fc2",
            "76f7a6da850b431a9b7eeea532792cc1",
            "9954b62bc2b544a39be3512b461932c9",
            "a72568f0c48540069637849db64f9eef",
            "74c65b89e4f94258bf72e7ec1de443f9",
            "aa089ed6e29b4b339e9ae1d8718d5dcc",
            "d0c3cc4d557f40ce964e5ec45e0a8a4f",
            "61df3c9978e04adc9e4ed08cfd2a5cce",
            "63d12422be26412da65ebb744b9012c8",
            "57cb7ac14d874fde9cd827ed50dde4e5",
            "6fa0b15ef2724dc5a3dabfedc80e03a5",
            "7e4673dcee18499baac20b282dc571df",
            "244893e7a9ff43a79559318b65555bf5",
            "f62587ab97b641c08513bdeb0c50519e",
            "ef8a2bdd1dbe4bd288f9638ad2122105",
            "a70b5dde64ce4dc59bb02c4d807e817d",
            "656b46a8658d4e91b6999b0a6f68fe62",
            "747c458da2aa4d9d969bb7592a641b76",
            "bc64227aca824b9792ca7c5f6d6562f6",
            "ba290ec6b36e44d7be4118f14808d924",
            "94cbe2cd19da4da5a38d1ca0ca68ef3e",
            "c09dd7c0ca1d4c2882f115612065d9d1",
            "67f3e29d318a4f8387356618df46db7b",
            "dd9c98aa8a304e4fbd4c8bdedeee9d83",
            "956c34c109b245e3942445bc9588f94f",
            "799ffde6203f4b54a3a3918a3ade7d52",
            "35812b8882e3439bace66929ace63c72",
            "9192966e45804db782a74a7cf02e77b0",
            "7dcda371ffc84567a881f1b4ab756530",
            "157c40444ce94978a2be6905b1c96beb",
            "d360fe4917a048e1ac9bacabeb552f03",
            "68fdfa5ecec64642a1b35eea26de55ba",
            "8db4295de2a844e6a4d000ee169e9c27",
            "65514d50c4f542148b5df419521fa821",
            "7282ac7c766740b1a6aaad9577584bde",
            "9c4e5bb679e040e1a56a74a24d86d9cf",
            "0cc2561b7f9640e69491d301f32a1cc5",
            "2424a0ffaed04d5e86db20993edcee0e",
            "65ee906c10f14791b5081515a511d083",
            "8c34d9ece26641f1a9d5236b579e300e",
            "30837a2bca73453f847cf890f3aee6d0",
            "ed131ce3aa8b46658d391fb8f649f153"
          ]
        },
        "outputId": "173a87ab-62cb-4901-c02b-ef3c0a65f63b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c89218112644c7e834a9cbd16f35832"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15df5ab7c5f847cfb91e05ffffe18a98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd18c2fdd6b34bb79ffaa6e8b2aea6f2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa2c5e53826e404fb415e4a4cb04fa14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "60530f2909704f11bd0965dade1de13a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "normalizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "57cb7ac14d874fde9cd827ed50dde4e5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "94cbe2cd19da4da5a38d1ca0ca68ef3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68fdfa5ecec64642a1b35eea26de55ba"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import AutoProcessor\n",
        "model_id = \"openai/whisper-small\"  # whisper-large-v3\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9iRIyn3Msekm"
      },
      "outputs": [],
      "source": [
        "\n",
        "text = \"This is a test sentence.\"\n",
        "encoded_input = processor.tokenizer(text)\n",
        "# print(encoded_input.keys())\n",
        "# print(processor.tokenizer.batch_decode(encoded_input[\"input_ids\"]))\n",
        "# # If you want to see all available tokens:\n",
        "# print(len(processor.tokenizer.get_vocab()))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    encoded_input.keys() -> KeysView({'input_ids': [50258, 50363, 5723, 307, 257, 1500, 8174, 13, 50257], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
        "\n",
        "    processor.tokenizer.batch_decode(encoded_input[\"input_ids\"]) -> ['<|startoftranscript|>', '<|notimestamps|>', 'This', ' is', ' a', ' test', ' sentence', '.', '<|endoftext|>']\n",
        "\n",
        "    processor.tokenizer.get_vocab() -> {'!': 0, '\"': 1, '#': 2, '$': 3, '%': 4, '&': 5, \"'\": 6, '(': 7, ')': 8, '*': 9, '+': 10, ',': 11, '-': 12, '.': 13, '/': 14, '0': 15, '1': 16, '2': 17,... upto 51864]"
      ],
      "metadata": {
        "id": "rwiEX74YPoZB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQiI8Bas1CCM"
      },
      "source": [
        "```python\n",
        "\n",
        "dataset_2 = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "dataset_2_sample = iter(dataset_2)\n",
        "sample_2 = next(dataset_2_sample)\n",
        "sample_2\n",
        "\n",
        "audio_data = Audio(processor.feature_extractor.sampling_rate)\n",
        "audio_data = np.array(sample_2[\"audio\"][\"array\"])\n",
        "\n",
        "audio, sr = librosa.load(audio_data, sr=sampling_rate)\n",
        "\n",
        "\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Tpeq8RufrxP4"
      },
      "outputs": [],
      "source": [
        "# dataset1 = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "# dataset1 = dataset1.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\n",
        "# sample = dataset1[\"audio\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "    Dataset({\n",
        "    features: ['file', 'audio', 'text', 'speaker_id', 'chapter_id', 'id'],\n",
        "    num_rows: 73\n",
        "    })\n",
        "\n",
        "    dataset[\"audio\"][0] -> {'path': '1272-128104-0000.flac',\n",
        "    'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
        "            0.0010376 ]),\n",
        "    'sampling_rate': 16000}"
      ],
      "metadata": {
        "id": "1P4P_DO0P5oH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "\n",
        "dataset_2 = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "dataset_2_sample = iter(dataset_2)\n",
        "sample_2 = next(dataset_2_sample)\n",
        "sample_2\n",
        "\n",
        "{'file': '/Users/sanchitgandhi/.cache/huggingface/datasets/downloads/extracted/aad76e6f21870761d7a8b9b34436f6f8db846546c68cb2d9388598d7a164fa4b/dev_clean/1272/128104/1272-128104-0000.flac',\n",
        " 'audio': {'path': '1272-128104-0000.flac',\n",
        "  'array': array([0.00238037, 0.0020752 , 0.00198364, ..., 0.00042725, 0.00057983,\n",
        "         0.0010376 ]),\n",
        "  'sampling_rate': 16000},\n",
        " 'text': 'MISTER QUILTER IS THE APOSTLE OF THE MIDDLE CLASSES AND WE ARE GLAD TO WELCOME HIS GOSPEL',\n",
        " 'speaker_id': 1272,\n",
        " 'chapter_id': 128104,\n",
        " 'id': '1272-128104-0000'}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "w7kOmco7X_nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using Onset Detection and Segmentation**:\n",
        "\n",
        "**Onset Detection**: Identify points in the audio where new sounds or events begin using libraries like librosa.\n",
        "\n",
        "**Segmentation**: Divide the audio into segments based on onset detection, treating each segment as a token.\n",
        "\n",
        "```python\n",
        "import librosa\n",
        "\n",
        "# Onset detection\n",
        "onsets = librosa.onset.onset_detect(y=audio_array, sr=sampling_rate)\n",
        "\n",
        "# Segmentation (example: dividing audio into segments based on onsets)\n",
        "segments = []\n",
        "start = 0\n",
        "for onset in onsets:\n",
        "    segments.append(audio_array[start:onset])\n",
        "    start = onset\n",
        "\n",
        "# Each segment in 'segments' can be considered a token\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "znk7lnRRe6Rb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJBj6h9f0d5r"
      },
      "source": [
        "```\n",
        "processor.feature_extractor -> WhisperFeatureExtractor {\n",
        "  \"chunk_length\": 30,\n",
        "  \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
        "  \"feature_size\": 80,\n",
        "  \"hop_length\": 160,\n",
        "  \"n_fft\": 400,\n",
        "  \"n_samples\": 480000,\n",
        "  \"nb_max_frames\": 3000,\n",
        "  \"padding_side\": \"right\",\n",
        "  \"padding_value\": 0.0,\n",
        "  \"processor_class\": \"WhisperProcessor\",\n",
        "  \"return_attention_mask\": false,\n",
        "  \"sampling_rate\": 16000\n",
        "}\n",
        "```\n",
        "\n",
        "Chunked Long-Form\n",
        "Whisper has a receptive field of 30-seconds. To transcribe audios longer than this, one of two long-form algorithms are required:\n",
        "\n",
        "**Sequential**: uses a \"sliding window\" for buffered inference, transcribing 30-second slices one after the other\n",
        "\n",
        "**Chunked**: splits long audio files into shorter ones (with a small overlap between segments), transcribes each segment independently, and stitches the resulting transcriptions at the boundaries\n",
        "\n",
        "\n",
        "**The sequential long-form algorithm should be used in either of the following scenarios**:\n",
        "\n",
        "  >Transcription accuracy is the most important factor, and speed is less of a consideration\n",
        "\n",
        "  >You are transcribing batches of long audio files, in which case the latency of sequential is comparable to chunked, while being up to 0.5% WER more accurate\n",
        "\n",
        "\n",
        "**Conversely, the chunked algorithm should be used when:**\n",
        "\n",
        "  >Transcription speed is the most important factor\n",
        "\n",
        "  >You are transcribing a single long audio file\n",
        "\n",
        "\n",
        "\n",
        ">By default, Transformers uses the sequential algorithm. To enable the chunked algorithm, pass the chunk_length_s parameter to the pipeline. For large-v3, a chunk length of 30-seconds is optimal.\n",
        "\n",
        ">To activate batching over long audio files, pass the argument batch_size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mtdD9uIIr2VJ"
      },
      "outputs": [],
      "source": [
        "inputs = processor(\n",
        "    sample[\"array\"],\n",
        "    sampling_rate=sample[\"sampling_rate\"],\n",
        "    return_tensors=\"pt\",\n",
        "    truncation=False,\n",
        "    padding=\"longest\",\n",
        "    return_attention_mask=True,\n",
        ")\n",
        "\n",
        "# Pad to 3000 frames\n",
        "max_frames = processor.feature_extractor.nb_max_frames\n",
        "inputs[\"input_features\"] = inputs[\"input_features\"][:, :max_frames]\n",
        "input_features = inputs.input_features\n",
        "input_features_padded = torch.nn.functional.pad(\n",
        "    input_features, (0, max_frames - input_features.shape[-1])\n",
        ")\n",
        "inputs[\"input_features\"] = input_features_padded\n",
        "\n",
        "inputs = inputs.to(device, dtype=torch_dtype)\n",
        "\n",
        "inputs = inputs.to(device, dtype=torch_dtype)\n",
        "# inputs[\"input_features\"] : (batch_size,feature_size,nb_max_frames) : torch.Size([1, 80, 3000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Wt7Rou0KpUmq"
      },
      "outputs": [],
      "source": [
        "gen_kwargs = {\n",
        "    \"max_new_tokens\": 445,  # Reduced to 445 to stay within the limit of 448\n",
        "    \"num_beams\": 1,\n",
        "    \"condition_on_prev_tokens\": False,\n",
        "    \"compression_ratio_threshold\": 1.35,  # zlib compression ratio threshold (in token space)\n",
        "    \"temperature\": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),\n",
        "    \"logprob_threshold\": -1.0,\n",
        "    \"no_speech_threshold\": 0.6,\n",
        "    \"return_timestamps\": True,\n",
        "    # \"max_initial_condition_len\": 150,\n",
        "    \"pad_token_id\": processor.tokenizer.pad_token_id,\n",
        "    \"eos_token_id\": processor.tokenizer.eos_token_id,\n",
        "    \"decoder_start_token_id\": processor.tokenizer.bos_token_id,\n",
        "    \"language\":'en'\n",
        "}\n",
        "\n",
        "pred_ids = model.generate(**inputs, **gen_kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    pred_ids : torch.Size([1, 26]) -> tensor([[50257, 50259, 50359, 50364,  2221,    13,  2326,   388,   391,   307,\n",
        "              264, 31467,   306,   295,   264, 10775,  9471,   279,    11,   293,\n",
        "              321,   366,  5404,   281,  2928, 50588, 50588,   702, 23163,     0,\n",
        "            50638]])"
      ],
      "metadata": {
        "id": "9lO_sod_QXMF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "PYRmEUcVr-Id"
      },
      "outputs": [],
      "source": [
        "pred_text = processor.batch_decode(pred_ids, skip_special_tokens=True, decode_with_timestamps=False)\n",
        "# [' Mr. Quilter is the Apostle of the Middle Classes, and we are glad to welcome his Gospel.']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ASR Using Pipeline\n",
        "\n",
        "**Warm-up** : In each warm-up step, it processes a copy of the sample with the pipeline, generating a sequence of 256 new tokens.\n",
        "\n",
        "**Fast Run** : After the warm-up steps, it processes the sample again with the pipeline, this time without specifying the number of new tokens to generate.\n",
        "\n",
        "\n",
        "**sdpa_kernel(SDPBackend.MATH)** It's the context manager, which ensures that the attention mechanism uses the math backend for its operations. This can help optimize the performance and stability of the model during these initial runs."
      ],
      "metadata": {
        "id": "Pe3o-ueoZ8wN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8-M9oF22sZ2w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975ecf66-64b5-40a0-96fe-d7af59635f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
            "/usr/local/lib/python3.12/dist-packages/torch/__init__.py:1617: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
            "  _C._set_float32_matmul_precision(precision)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
        "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "model_id = \"openai/whisper-small\"  # whisper-large-v3\n",
        "\n",
        "torch.set_float32_matmul_precision(\"high\")\n",
        "# won't work with cuda\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scale-Product-Attention (SDPA) - can also use flash_attention_2\n",
        "model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
        "    model_id, dtype=torch_dtype, low_cpu_mem_usage=True,\n",
        "    attn_implementation=\"sdpa\")\n",
        "\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qB0sJ2V1-XCO",
        "outputId": "b1aa93c4-cb93-48b5-cf6f-2ed7578ca552"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WhisperForConditionalGeneration(\n",
              "  (model): WhisperModel(\n",
              "    (encoder): WhisperEncoder(\n",
              "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
              "      (embed_positions): Embedding(1500, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperEncoderLayer(\n",
              "          (self_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): WhisperDecoder(\n",
              "      (embed_tokens): Embedding(51865, 768, padding_idx=50257)\n",
              "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x WhisperDecoderLayer(\n",
              "          (self_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): WhisperAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (proj_out): Linear(in_features=768, out_features=51865, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=model,\n",
        "    tokenizer=processor.tokenizer,\n",
        "    feature_extractor=processor.feature_extractor,\n",
        "    # chunk_length_s=30,\n",
        "    batch_size=16,  # batch size for inference - will make inference slow\n",
        "    dtype=torch_dtype,\n",
        "    device=device,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5YKO52v9fwk",
        "outputId": "03a50a58-9f34-48d6-962d-0ac92126ef25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio, load_dataset\n",
        "\n",
        "dataset2 = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "dataset2 = dataset2.cast_column(\"audio\", Audio(processor.feature_extractor.sampling_rate))\n",
        "sample2 = dataset2[\"audio\"][0]"
      ],
      "metadata": {
        "id": "UrW2q-DL-roa"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 warmup steps\n",
        "for _ in tqdm(range(2), desc=\"Warm-up step\"):\n",
        "    with sdpa_kernel(SDPBackend.MATH):\n",
        "        result = pipe(sample2,\n",
        "                      generate_kwargs={\"min_new_tokens\": 256, \"max_new_tokens\": 256})\n",
        "\n",
        "# fast run\n",
        "with sdpa_kernel(SDPBackend.MATH):\n",
        "    result = pipe(sample2)\n",
        "\n",
        "# result : {'text': ' Mr. Quilter is the Apostle of the Middle Classes, and we are glad to welcome his Gospel.'}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtM0WvGM8crV",
        "outputId": "216eed8e-7c5e-4af4-e162-e239508d7bf8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rWarm-up step:   0%|          | 0/2 [00:00<?, ?it/s]`return_token_timestamps` is deprecated for WhisperFeatureExtractor and will be removed in Transformers v5. Use `return_attention_mask` instead, as the number of frames can be inferred from it.\n",
            "Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
            "Transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English. This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`. See https://github.com/huggingface/transformers/pull/28687 for more details.\n",
            "Warm-up step: 100%|██████████| 2/2 [04:27<00:00, 133.59s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Whisper Tokens\n",
        "\n",
        "**How it works**: These tokenizers have been trained on large datasets and can efficiently **convert audio into discrete tokens that capture phonetic or acoustic information**."
      ],
      "metadata": {
        "id": "zC9NH-eTeqNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note** : TypeError: The current model class (WhisperModel) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq']."
      ],
      "metadata": {
        "id": "I1tWMcwduLLW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "import torch\n",
        "import librosa\n",
        "\n",
        "# Load the processor and model\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# Load and preprocess the audio\n",
        "audio_path = '/content/10s_joke.mp3'\n",
        "audio, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "# Preprocess the audio to match Whisper input requirements\n",
        "input_features = processor(audio, return_tensors=\"pt\",sampling_rate=16000).input_features\n",
        "# input_features.shape = torch.Size([1, 80, 3000])\n",
        "\n",
        "# Pass the preprocessed audio features to the model and generate the logits\n",
        "with torch.no_grad():\n",
        "    predicted_ids = model.generate(input_features)\n",
        "\n",
        "# Convert the predicted IDs (tokens) back to text using the processor\n",
        "transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True,language='en')\n",
        "tokens = predicted_ids  # These are the token IDs predicted by the model\n",
        "\n",
        "# print(\"Transcription:\", transcription)\n",
        "# print(\"Tokens:\", tokens)\n",
        "# print(\"Predicted Tokens\",processor.tokenizer.batch_decode(tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiQKWkIbtS4-",
        "outputId": "4165998c-9801-4c64-99a4-d5f1eb293039"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "    Transcription: [' Hey buddy, how was school? Everybody was mocking me. Everybody was mocking you? Everybody was mocking you?']\n",
        "    Tokens: tensor([[50258, 50259, 50359, 50363,  1911, 10340,    11,   577,   390,  1395,\n",
        "                30,  7646,   390, 49792,   385,    13,  7646,   390, 49792,   291,\n",
        "                30,  7646,   390, 49792,   291,    30]])\n",
        "    Predicted Tokens ['<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Hey buddy, how was school? Everybody was mocking me. Everybody was mocking you? Everybody was mocking you?']"
      ],
      "metadata": {
        "id": "6_IRe-qpQwbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Direct Encoder Output"
      ],
      "metadata": {
        "id": "9wr7EsXbb2f3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor, WhisperModel,WhisperForConditionalGeneration\n",
        "import torch\n",
        "import librosa\n",
        "\n",
        "# Load the processor and model\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperModel.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# Load and preprocess the audio\n",
        "audio_path = '/content/10s_joke.mp3'\n",
        "audio, sr = librosa.load(audio_path, sr=16000)\n",
        "\n",
        "# Preprocess the audio to match Whisper input requirements\n",
        "input_features = processor(audio, return_tensors=\"pt\",sampling_rate=16000).input_features\n",
        "# input_features.shape [batch_size,feature_size,nb_max_frames] : torch.Size([1, 80, 3000])\n",
        "# Pass through the model to get hidden states (embeddings)\n",
        "with torch.no_grad():\n",
        "    # Directly access the encoder using model.encoder\n",
        "    encoder_outputs = model.encoder(input_features)\n",
        "\n",
        "# Extract the embeddings from the last hidden layer\n",
        "embeddings = encoder_outputs.last_hidden_state # Shape: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "# Optionally, get a single embedding by averaging over the time dimension\n",
        "audio_embedding = embeddings.mean(dim=1)  # Shape: (batch_size, hidden_size)\n",
        "# embeddings.shape [batch_size, sequence_length, hidden_size] : torch.Size([1, 1500, 768])\n",
        "# audio_embedding.shape [batch_size, hidden_size] : torch.Size([1, 768])"
      ],
      "metadata": {
        "id": "NstVhnmfsdn-"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PJtcXB5aiAgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get Decoder output"
      ],
      "metadata": {
        "id": "PspxS1LhkxT3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer, WhisperForConditionalGeneration\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")\n",
        "\n",
        "# Generate token IDs for decoder input\n",
        "input_ids = torch.tensor([[tokenizer.bos_token_id]])  # # BOS token ID, shape: (1, 1)\n",
        "\n",
        "# Instead of calling model.decoder, call model directly with both inputs\n",
        "# **Change:** Remove input_ids and use decoder_input_ids instead\n",
        "with torch.no_grad():\n",
        "    # **Add output_hidden_states=True to get decoder hidden states**\n",
        "    decoder_outputs = model(decoder_input_ids=input_ids,\n",
        "                            encoder_outputs=encoder_outputs,\n",
        "                            output_hidden_states=True) # Use 'encoder_outputs'\n",
        "\n",
        "# Access the last hidden state from the decoder output\n",
        "# decoder_outputs is a Seq2SeqLMOutput object, its attributes depend on the model and inputs\n",
        "# **Change:** decoder_outputs.last_hidden_state to decoder_outputs.decoder_hidden_states[-1]\n",
        "# to get the last hidden state of the decoder.\n",
        "last_hidden_state = decoder_outputs.decoder_hidden_states[-1]\n",
        "# decoder_outputs.keys() : odict_keys(['logits', 'past_key_values', 'decoder_hidden_states', 'encoder_last_hidden_state'])\n",
        "# last_hidden_state.shape (batch_size, sequence_length, hidden_size) : torch.Size([1, 1, 768])\n",
        "\n",
        "last_token_hidden_state = last_hidden_state[:, -1, :]\n",
        "# last_token_hidden_state.shape : torch.Size([1, 768])\n",
        "\n",
        "# Access logits from decoder_outputs using .logits\n",
        "logits = decoder_outputs.logits[:, -1, :]  # shape: (batch_size, vocab_size)\n",
        "probabilities = torch.softmax(logits, dim=-1)  # shape: (batch_size, vocab_size)\n",
        "predicted_token_id = torch.argmax(probabilities, dim=-1) # shape: (batch_size)\n",
        "# predicted_token_id : tensor([50259])\n",
        "# processor.tokenizer.batch_decode(predicted_token_id) : ['<|en|>']"
      ],
      "metadata": {
        "id": "2ZPmJ09keAxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Accessing and Decoding decoder_hidden_states ---\n",
        "# Get all decoder hidden states\n",
        "# all_decoder_hidden_states = decoder_outputs.decoder_hidden_states  # List of tensors\n",
        "\n",
        "# Convert each hidden state to token IDs (argmax over vocab) and then decode to text\n",
        "for i, hidden_state in enumerate(all_decoder_hidden_states):\n",
        "    # Reshape to (batch_size * sequence_length, hidden_size) for argmax\n",
        "    hidden_state_reshaped = hidden_state.reshape(-1, hidden_state.shape[-1])\n",
        "\n",
        "    # Get predicted token IDs by taking argmax over vocabulary dimension\n",
        "    predicted_token_ids = torch.argmax(hidden_state_reshaped, dim=-1)\n",
        "\n",
        "    # Convert predicted_token_ids to a list to pass to batch_decode\n",
        "    predicted_token_ids = predicted_token_ids.tolist() # Change here\n",
        "\n",
        "    # Decode token IDs to text using the tokenizer\n",
        "    decoded_tokens = tokenizer.batch_decode(predicted_token_ids, skip_special_tokens=True)\n",
        "\n",
        "    print(f\"Decoder Hidden State {i}: {decoded_tokens}\")\n",
        "\n",
        "\n",
        "# --- Accessing and Decoding encoder_last_hidden_state ---\n",
        "encoder_last_hidden_state = encoder_outputs.last_hidden_state  # shape: (batch_size, sequence_length, hidden_size)\n",
        "\n",
        "# Reshape to (batch_size * sequence_length, hidden_size) for argmax\n",
        "encoder_hidden_state_reshaped = encoder_last_hidden_state.reshape(-1, encoder_last_hidden_state.shape[-1])\n",
        "\n",
        "# Get predicted token IDs by taking argmax over vocabulary dimension\n",
        "predicted_token_ids_encoder = torch.argmax(encoder_hidden_state_reshaped, dim=-1)\n",
        "\n",
        "# Convert predicted_token_ids_encoder to a list before decoding\n",
        "predicted_token_ids_encoder = predicted_token_ids_encoder.tolist()  # Change here\n",
        "\n",
        "# Decode token IDs to text using the tokenizer\n",
        "decoded_tokens_encoder = tokenizer.batch_decode(predicted_token_ids_encoder, skip_special_tokens=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "rXj3dAKZeA0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Encoder Last Hidden State: {decoded_tokens_encoder}\")"
      ],
      "metadata": {
        "id": "XZ4Nr0pH4Dri"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
