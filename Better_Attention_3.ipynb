{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMEsxpOjhJaH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multi-head Latent Attention (MLA)"
      ],
      "metadata": {
        "id": "0lAmjAcujGgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sources:**\n",
        "\n",
        "* DeepSeek v2 [https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434) (05/2024)\n",
        "* DeepSeek v3 [https://arxiv.org/abs/2412.19437](https://arxiv.org/abs/2412.19437) (12/2024)\n",
        "\n",
        "**Visual Comparison of Attention Mechanisms**\n",
        "\n",
        "The image displays a diagram comparing how Queries, Keys, and Values are handled:\n",
        "\n",
        "* **MHA (Multi-Head Attention):** Unique Keys and Values for every Query head.\n",
        "* **GQA (Grouped-Query Attention):** Keys and Values are shared across groups of Query heads.\n",
        "* **MQA (Multi-Query Attention):** All Query heads share a single set of Keys and Values.\n",
        "* **MLA (Multi-Head Latent Attention):** Uses a **Compressed Latent KV** vector which is projected into Keys and Values, significantly reducing the cached data.\n",
        "\n",
        "**Key Performance & Features**\n",
        "\n",
        "* **Implementation:** Used in DeepSeek v2 and v3.\n",
        "* **Caching Strategy:** K and V matrices are **not cached**; instead, a **low-rank representation** learned during training is cached.\n",
        "* **Efficiency:** * Much lower KV cache usage (**90%+ savings**).\n",
        "* **5-6x inference speedup**.\n",
        "\n",
        "\n",
        "* **Quality:** Accuracy is **higher than MHA**.\n",
        "* **Reference:** Detailed in Appendix D.1 of the DeepSeek v2 paper.\n",
        "\n",
        "---\n",
        "\n",
        "**Table 1: KV Cache Comparison**\n",
        "\n",
        "Comparison of the KV cache per token among different attention mechanisms (where  denotes number of layers).\n",
        "\n",
        "\\begin{array}{l|l|l}\n",
        "\\text{Attention Mechanism} &\n",
        "\\text{KV Cache per Token (\\# Element)} &\n",
        "\\text{Capability} \\\\\n",
        "\\hline\n",
        "\\text{Multi-Head Attention (MHA)} &\n",
        "2 n_h d_h l &\n",
        "\\text{Strong} \\\\\n",
        "\\text{Grouped-Query Attention (GQA)} &\n",
        "2 n_g d_h l &\n",
        "\\text{Moderate} \\\\\n",
        "\\text{Multi-Query Attention (MQA)} &\n",
        "2 d_h l &\n",
        "\\text{Weak} \\\\\n",
        "\\textbf{MLA (Ours)} &\n",
        "(d_c + d_h^{R})\\, l \\approx \\frac{9}{2} d_h l &\n",
        "\\textbf{Stronger}\n",
        "\\end{array}\n",
        "\n",
        "**Note from text:** $n_h$ is the number of heads, $d_h$ is dimension per attention head,$l$ denotes the number of layers,$n_g$  is number of groups in GQA,and $d_c$ and $d_h^R$ denote the KV compression dimension and the per-head dimension of the decoupled queries and key in MLA, respectively. For DeepSeek-V2,$d_c = 4d_h$ and $d_h^R = \\frac{d_h}{2}$.\n",
        "\n"
      ],
      "metadata": {
        "id": "RfTsTqd4iAzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\begin{array}{|l|c|c|l|}\n",
        "\\hline\n",
        "\\textbf{Tensor} & \\textbf{Dimensions} & \\textbf{Example} & \\textbf{Purpose} \\\\ \\hline\n",
        "X: \\text{input embeddings} & N \\times d_{\\text{hidden}} & 11 \\times 512 & \\text{The input sequence, in embedded form} \\\\ \\hline\n",
        "W_{Qi}: \\text{query weight matrix} & & & d_{\\text{mha}} = d_{\\text{hidden}} / \\text{number of attention heads (here, 8)} \\\\\n",
        "W_{Ki}: \\text{key weight matrix} & d_{\\text{hidden}} \\times d_{\\text{mha}} & 512 \\times 64 & \\\\\n",
        "W_{Vi}: \\text{value weight matrix} & & & \\text{Each head has its own weight matrices, which learn different features.} \\\\\n",
        "W_0: \\text{feed-forward weight matrix} & & & \\\\ \\hline\n",
        "W_{i \\text{ down}}: \\text{down-projection matrix} & d_{\\text{mha}} \\times d_{\\text{mha\\_latent}} & 64 \\times 4 & d_{\\text{latent}} \\text{ should be much smaller than } d_{\\text{hidden}} \\text{ (here, 32)} \\\\\n",
        "W_{\\text{up}}: \\text{up-projection matrix} & d_{\\text{latent}} \\times d_{\\text{hidden}} & 32 \\times 512 & d_{\\text{mha\\_latent}} = d_{\\text{latent}} / \\text{number of attention heads (here, 8) = 4} \\\\ \\hline\n",
        "Q_i = X W_{Qi}: \\text{query matrix} & N \\times d_{\\text{mha}} & 11 \\times 64 & \\text{All heads run this in parallel.} \\\\\n",
        "K_i = X W_{Ki} W_{i \\text{ down}}: \\text{key matrix} & N \\times d_{\\text{mha\\_latent}} & 11 \\times 4 & K_i \\text{ and } V_i \\text{ are stored in the KV cache and are } d_{\\text{hidden}}/d_{\\text{latent}} \\text{ times smaller (here, 16).} \\\\\n",
        "V_i = X W_{Vi} W_{i \\text{ down}}: \\text{value matrix} & N \\times d_{\\text{mha\\_latent}} & 11 \\times 4 & \\text{The tradeoff is an extra matmut to compute } K_i \\text{ and } V_i. \\text{ DeepSeek uses different down-projection matrices for K and V.} \\\\ \\hline\n",
        "Q_i W_{i \\text{ down}} K_i^T: \\text{attention scores} & N \\times N & 11 \\times 11 & \\text{All heads run this in parallel. } Q_i \\text{ is down-projected for this calculation only (possibly with its own matrix).} \\\\ \\hline\n",
        "\\text{softmax}\\left( \\frac{\\text{scores}}{\\sqrt{d_k}} \\right) V & N \\times d_{\\text{mha\\_latent}} & 11 \\times 4 & \\text{All heads run this in parallel} \\\\ \\hline\n",
        "\\text{Concatenate head outputs} & N \\times d_{\\text{latent}} & 11 \\times 32 & \\\\ \\hline\n",
        "\\text{Outputs} \\times W_{\\text{up}} & N \\times d_{\\text{hidden}} & 11 \\times 512 & \\text{Bring back the output to the initial dimension} \\\\ \\hline\n",
        "\\text{Attention weights} \\times W_0 & N \\times d_{\\text{hidden}} & 11 \\times 512 & \\text{Capture additional interactions across the sequence} \\\\ \\hline\n",
        "\\end{array}$$\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "CuwvK_TtiCRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### From Attention Outputs to Text Generation Table"
      ],
      "metadata": {
        "id": "W6SrRIapmSHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\\begin{array}{|l|c|c|l|}\n",
        "\\hline\n",
        "\\textbf{Tensor} & \\textbf{Dimensions} & \\textbf{Example} & \\textbf{Purpose} \\\\ \\hline\n",
        "\\text{Attention output for the input sequence} & N \\times d_{\\text{hidden}} & 11 \\times 512 & \\text{Updated token embeddings after considering the context} \\\\\n",
        "\\text{(aka pre-fill), stored in the KV cache} & & & \\text{of all other tokens.} \\\\ \\hline\n",
        "\\text{Retrieve the attention output for the last} & 1 \\times d_{\\text{hidden}} & 1 \\times 512 & \\text{} \\\\\n",
        "\\text{token in the input sequence} & & & \\\\ \\hline\n",
        "W_{\\text{output}}: \\text{linear layer} & V \\times d_{\\text{hidden}} & 100,000 \\times 512 & V: \\text{vocabulary size (here, 100,000)} \\\\\n",
        "\\text{(aka projection layer)} & & & \\\\ \\hline\n",
        "\\text{Logits} = \\text{attention output} \\times W_{\\text{output}}^T & 1 \\times V & 1 \\times 100,000 & \\text{Raw scores for all tokens in the vocabulary} \\\\ \\hline\n",
        "\\text{softmax(Logits)} & 1 \\times V & 1 \\times 100,000 & \\text{Turn token scores into token probabilities} \\\\ \\hline\n",
        "\\text{Decode the token} & 1 \\text{ token} & 1 & \\text{Greedy decoding: pick the token with the highest probability} \\\\\n",
        "& & & \\text{Top-k sampling: pick a token from the } k \\text{ most likely tokens} \\\\\n",
        "& & & \\text{Top-p decoding: pick a token from the smallest subset of tokens} \\\\\n",
        "& & & \\text{such that their cumulative probability exceeds the } p \\text{ threshold} \\\\ \\hline\n",
        "\\text{Use the new token as the next input} & & & \\text{} \\\\ \\hline\n",
        "\\text{Repeat until a stopping condition is met} & & & \\text{End-of-sentence token, or maximum number of output tokens} \\\\ \\hline\n",
        "\\end{array}$$"
      ],
      "metadata": {
        "id": "KuosxVcCikv0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flash Attention"
      ],
      "metadata": {
        "id": "sx0r9PjsETMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The original Flash Attention introduced tiling and SRAM utilization to avoid HBM bottlenecks.\n",
        "\n",
        "- Avoids repeated reading and writing of the attention matrix to HBM by loading Q and K once and keeping intermediate results in SRAM.\n",
        "\n",
        "- Computes attention scores (P) incrementally in SRAM using tiling before writing back to HBM.\n",
        "\n",
        "- Parallelizes computations over both the batch size and the number of heads.\n",
        "\n",
        "- Reduces memory complexity to linear, resulting in 2-4x speed improvements and 10-20x memory savings.\n",
        "\n",
        "- Optimized for both forward and backward passes to accelerate model training.\n",
        "\n",
        "- Integrated into Hugging Face Text Generation Inference (TGI)"
      ],
      "metadata": {
        "id": "pmyBTvwmEXE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Flash Attention 2**"
      ],
      "metadata": {
        "id": "_9gUAOleF-Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Flash Attention 2 focuses on maximizing GPU throughput and increasing parallelism compared to the original version.\n",
        "\n",
        "- Minimizes non-matmul operations to maximize GPU throughput.\n",
        "\n",
        "- Optimizes operations specifically for Multi-Query Attention (MQA) and Grouped-Query Attention (GQA).\n",
        "\n",
        "- Increases parallelism across the sequence length.\n",
        "\n",
        "- Optimizes both prompt processing (prefill) and text generation.\n",
        "\n",
        "- Performs 2x faster than Flash Attention and up to 9x faster than standard attention.\n"
      ],
      "metadata": {
        "id": "UK5OKrvqEge7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Paged Attention"
      ],
      "metadata": {
        "id": "BaxBgQeoGBUs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paged Attention, a memory management technique inspired by operating systems to improve GPU efficiency for LLM inference.\n",
        "\n",
        "**Challenges Addressed**\n",
        "- The KV cache memory size changes dynamically for every inference request.\n",
        "- Traditional allocation leads to GPU memory fragmentation, which wastes space and limits batch size scalability.\n",
        "\n",
        "**Core Mechanism**\n",
        "- Paged Attention partitions the KV cache into fixed-size, memory-aligned blocks called \"pages,\" similar to virtual memory in operating systems.\n",
        "- This page-based allocation significantly reduces both internal and external memory fragmentation.\n",
        "\n",
        "**Availability**\n",
        "- It is the core technology implemented in the [**vLLM project**](https://github.com/vllm-project/vllm).\n",
        "- Paged Attention is also available in **Hugging Face Text Generation Inference (TGI)**."
      ],
      "metadata": {
        "id": "bnYT26fwKhDY"
      }
    }
  ]
}